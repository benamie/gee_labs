{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benamie/gee_labs/blob/main/01_BackgroundIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "2699296e-6725-4899-908b-7d3fc81ce27a",
        "deepnote_cell_height": 242,
        "deepnote_cell_type": "markdown",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 61,
        "execution_start": 1648998113209,
        "source_hash": "38e8d8f7",
        "tags": [],
        "id": "cOjsrjFqHpki"
      },
      "source": [
        "# Digital Images\n",
        "\n",
        "## Overview\n",
        "\n",
        "The purpose of this lab is to introduce you to the core concepts of remote sensing. We will cover digital images, datum and projections, and the different types of resolution: spatial, spectral, temporal and radiometric. We will also cover some of the most well-known satellite platforms that we will be working with. At the completion of this lab, you will be able to work within Google Earth Engine to select the data that is the best fit for your use case, visualize the data and begin to extract insight from it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "729882b55a2544f091f74589c38ec77c",
        "deepnote_cell_height": 263,
        "deepnote_cell_type": "markdown",
        "tags": [],
        "id": "sboQRQ7HHpkl"
      },
      "source": [
        "#### Learning Outcomes\n",
        "\n",
        "1. Understand and describe the following terms:\n",
        "   - Digital image\n",
        "   - Datum\n",
        "   - Projection\n",
        "   - Resolution (spatial, spectral, temporal, radiometric)\n",
        "2. Navigate the Google Earth Engine console to gather information about the imagery contained within a satellite platform\n",
        "3. Visualize a digital image within Google Earth Engine and use inspector to look at pixel values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00002-17a4c969-df57-477b-9754-1c032025d04e",
        "deepnote_cell_height": 271,
        "deepnote_cell_type": "markdown",
        "id": "M4V-SVcVHpkm"
      },
      "source": [
        "## Digital Image\n",
        "\n",
        "A digital image is a matrix of equally-sized square pixels that are each defined by two main attributes:\n",
        "\n",
        "1. The position within the matrix, as defined by row, column and layer\n",
        "2. A value associated with that position\n",
        "\n",
        "In the context of geospatial imagery, we will refer to these pixel-based data structures as 'raster', as opposed to 'vector' data (points, lines, polygons). While vector and raster data both work in conjunction with one another, they have different attributes and characteristics. Before we discuss geospatial raster imagery, let's understand how a regular photograph is created. All of the images below were used from example photographs in the documentation in MatLab and OpenCV. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00003-fdd6d647-f8a0-443e-9588-29776e617d66",
        "deepnote_cell_height": 1061,
        "deepnote_cell_type": "markdown",
        "id": "ZepfP5-wHpkn"
      },
      "source": [
        "#### One Layer Grayscale\n",
        "\n",
        "Let's start with a grayscale image of some peppers. This image is a rectangle that contains 384 rows and 512 columns of pixels - because it is greyscale, there is only one brightness magnitude value (between 0 and 255) for each position. While humans see shapes, hues and definition, a computer is only recording a brightness value for each pixel. \n",
        "\n",
        "![Grayscale Image](https://github.com/ghidora77/03_GEE_Labs_DSPG/blob/main/im/im_01_01.png?raw=true)\n",
        "\n",
        "Below is a small segment of the matrix of values of the greyscale image between rows 50 and 60 and columns 50 and 60. Note that when working with imagery, the row/column position starts on the top left. Using our row/column designation:\n",
        "\n",
        "* `greyscale(54, 50)`  has a value of 52\n",
        "* `greyscale(50, 54)` has a value of 49\n",
        "* `greyscale(60, 60)` has a value of 47\n",
        "\n",
        "![Matrix of Grayscale Image](https://github.com/ghidora77/03_GEE_Labs_DSPG/blob/main/im/im_01_02.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00004-f02082bc-c560-4e89-a3ae-d38cf5c0180e",
        "deepnote_cell_height": 626,
        "deepnote_cell_type": "markdown",
        "id": "GwTB8NlfHpko"
      },
      "source": [
        "#### Three Layer Color Image\n",
        "\n",
        "The image is still the same, although this now has color. The number of rows, columns and the size of each pixel remain the same, but unlike the greyscale image, we now have three layers, or bands. Each band represents the value in each of the three primary colors: red, green, blue. If we look at the size of our matrix, it is now 384x512x3. For each row and column position, we now have 3 separate values between 0 and 255, which blend together into a color that we, as humans, can process. \n",
        "\n",
        "![Color Image](https://github.com/ghidora77/03_GEE_Labs_DSPG/blob/main/im/im_01_03.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00005-9cef6cb9-0f6c-4bb6-9d37-b386b906c4f7",
        "deepnote_cell_height": 2321,
        "deepnote_cell_type": "markdown",
        "id": "afxJV7WMHpkp"
      },
      "source": [
        "#### Extension to Geospatial Imagery\n",
        "\n",
        "Geospatial imagery poses two additional complications on top of a traditional color photograph. For analysis, we need to be able to tie our imagery to the real world. In the image of peppers, each pixel was built on an arbitrary axis of rows and columns that start at zero. However, this gives us no meaningful information about where in the world the red bell pepper is located. With geospatial imagery, we need to associate pixels with location. We need to know the exact size of the pixel, and position on earth. The high-resolution imagery below is produced by the 'National Agriculture Imagery Program'. This imagery has a red, green and blue value and a latitude and longitude (-74.01, 40.72), in addition to a size (each pixel represents 1 meter by 1 meter). With this information, we can associate our pixel values with location on earth (New York), and aggregate the information we need. \n",
        "\n",
        "![Pixel Location](https://github.com/ghidora77/03_GEE_Labs_DSPG/blob/main/im/im_01_04.png?raw=true)\n",
        "\n",
        "The other complexity we are faced with is that satellite imagery often has many layers. While our image of peppers had only a red, green and blue value, many satellite platforms are equipped to provide much more information. Most platforms have a value in the near infra-red range - while others have numerous bands, with different scales and resolutions. For instance, the Landsat 8 sensor has eleven bands capturing information from eleven different portions of the electromagentic spectrum, including near infrared (NIR) and thermal bands that are invisible to the human eye. Many Machine Learning projects, which we will explore in later labs, involve normalizing or transforming the information contained within each of these layers. Note that while each pixel size must be the same within each individual layer, the layers can be different. For instance, a satellite platform may have 5 meter spatial resolution in the red/green/blue range, but 60m resolution in the near infra-red range. While visualizing satellite imagery as a traditional photograph is a good starting point, there's much more information that we can incorporate into our analysis. We often build false or pseudocolor images by utilizing different combintions of bands, or we can focus in on certain infrared signatures to detect asphalt roads and roofs. The possibilities of analysis within remote sensing are endless, but this also leads to complications. \n",
        "\n",
        "![Spectral Band Documentation](https://github.com/ghidora77/03_GEE_Labs_DSPG/blob/main/im/im_01_05.png?raw=true)\n",
        "\n",
        "As said before, digital images are often referred to as 'raster' data. ESRI, makers of ArcGIS has an excellent overview of using raster imagery in geospatial analysis featured [here](https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/what-is-raster-data.htm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00006-39a8c0b8-a317-4f20-b9e6-86ee5cb52c50",
        "deepnote_cell_height": 610,
        "deepnote_cell_type": "markdown",
        "id": "wE_ntr1HHpkq"
      },
      "source": [
        "### From Digital Image to Geospatial Image\n",
        "\n",
        "To make the connection between our satellite imagery and real-world location even more complicated is the fact that a digital image is a flat, square surface - the earth is spherical. \n",
        "\n",
        "To make use remote sensing imagery, we need to align the pixels in our image to a real-world location. There's quite a bit of mathematics involved in this process, but we will focus on two main components - establishing a Geographic Coordinate System (GCS) and a Projected Coordinate System (PCS).\n",
        "\n",
        "The GCS defines the spherical location of the image whereas the PCS defines how the grid around that location is constructed. Because the earth is not a perfect sphere, there are different GCS for different regions, such as 'North American Datum: 83' which is used to accurately define North America, and 'World Geodetic System of 1984', which is used globally. \n",
        "\n",
        "The PCS then constructs a flat grid around the GCS to create a relationship between each pixel of a 2-dimensional image to the corresponding area on the world. Some of the common PCS formats include EPSG, Albers Conic, Lambert, Eckert, Equidistant, etc. Different types of PCS's are designed for different use cases, as the needs of a petroleum engineer working over a few square miles will differ from than a climate change researcher measuring global change. Much of the discussion of defining GCS is outside the scope of this course, but the important thing to note is that a GCS defines the starting point for a projection.\n",
        "\n",
        "ESRI has an [article](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/gcs_vs_pcs/) discussing the difference between GCS and PCS that provides further context on this relationship. While you should be aware of these two major terms - especially when you intend to run analysis on the data you download from GEE in another system, such as R, Python, or ArcGIS - GEE takes care of much of the complexity of these differences behind the scenes. Further documentation on the GEE methodology can be found [here](https://developers.google.com/earth-engine/guides/projections). In our first exercise, we will show you how to identify the PCS so you can understand the underlying structure. \n",
        "\n",
        "Understanding the bands available in your datasets, identifying which bands are necessary (and appropriate) for your analysis, and ensuring that these data represent consistent spatial locations is essential. While GEE simplifies many complex calculations behind the scenes, this lab will help us unpack the products available to us and their essential characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00007-459611fa-7101-4dfa-a2c4-360dee403dfb",
        "deepnote_cell_height": 1667.734375,
        "deepnote_cell_type": "markdown",
        "id": "xR4QtijpHpkr"
      },
      "source": [
        "### Visualize a Digital Image\n",
        "\n",
        "Let’s view a digital image in GEE to better understand this concept. There are three major ways to import imagery within GEE. \n",
        "\n",
        "1. You can navigate to the GEE [datasets](https://developers.google.com/earth-engine/datasets/) page, choose the image collection you would like to work with and import the code example (click the button called `import`), which is normally located at the bottom of each dataset page. This code example is a standalone code chunk that will correctly visualize the data, and is an excellent way to get an understanding for the different satellite platforms - feel free to explore some datasets you are interested in, change variables and inputs to see what happens. \n",
        "\n",
        "![Dataset Code Example](https://github.com/ghidora77/03_GEE_Labs_DSPG/blob/main/im/im_01_06.png?raw=true)\n",
        "\n",
        "2. In the search bar of the code editor, you can search for the specific imagery you are looking for. When you click on it, a pop-up window will come up that allows you to either import the image directly (bottom right) or copy the path to the image collection (left-hand side). They both work the same way, using the import button will incorporate it into the variable list directly (separated top section of the editor), where you have to specifically define the variable if you copy the path to the image collection. \n",
        "\n",
        "![Image Collection Snippet](https://github.com/ghidora77/03_GEE_Labs_DSPG/blob/main/im/im_01_07.png?raw=true)\n",
        "\n",
        "In the map window of GEE, click on the `point` geometry tool using the [geometry drawing tools](https://developers.google.com/earth-engine/playground#geometry-tools) to define your area of interest. For the purpose of consistency in this exercise, place a point on the Virginia Tech Drillfield, which will bring you roughly to (-80.42, 37.23). As a reminder, you can find more information on geometry drawing tools in GEE’s Guides. Name the import `point`.\n",
        "\n",
        "> Note: some programming languages and frameworks read in latitude and longitude differently - Most read in the values as longitude / latitude. Double check your values, if you are importing data from Google Maps, you will have to switch the latitude and longitude when using within GEE\n",
        "\n",
        "Import NAIP imagery by searching for 'naip' and choosing the *'NAIP: National Agriculture Imagery Program'* raster dataset. Name the import `naip`. \n",
        "\n",
        "Get a single, recent NAIP image over your study area and inspect it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00008-e16bae87-1a84-430d-bdc9-402e10ead006",
        "deepnote_cell_height": 261,
        "deepnote_cell_type": "code",
        "deepnote_output_heights": [
          382
        ],
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 5305,
        "execution_start": 1654104787385,
        "source_hash": "4c7ed9a4",
        "id": "uT2gpvXgHpks"
      },
      "outputs": [],
      "source": [
        "#!pip install geemap\n",
        "import ee, geemap, pprint\n",
        "#ee.Authenticate()\n",
        "def build_map(lat, lon, zoom, vizParams, image, name):\n",
        "    map = geemap.Map(center = [lat, lon], zoom = zoom)\n",
        "    map.addLayer(image, vizParams, name)\n",
        "    return map\n",
        "\n",
        "# Initialize the Earth Engine module.\n",
        "ee.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00009-6f0c617c-713c-487b-8bb3-f4c0a01c3fd5",
        "deepnote_cell_height": 1163,
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 2083,
        "execution_start": 1654104792704,
        "source_hash": "7f22a652",
        "id": "QzHipbBPHpku",
        "outputId": "6dacc082-3cce-4e20-f95b-f9599bfb5930",
        "colab": {
          "referenced_widgets": [
            "d95d827e50c54841aba33637de34c7af"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d95d827e50c54841aba33637de34c7af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map(center=[37.22, -80.42], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(childr…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "lat = 37.22; lon = -80.42\n",
        "zoom = 16\n",
        "image_collection_name = \"USDA/NAIP/DOQQ\"\n",
        "date_start = '2019-01-01'\n",
        "date_end = '2019-12-31'\n",
        "name = 'NAIP'\n",
        "point = ee.Geometry.Point([lon, lat])\n",
        "\n",
        "image = (\n",
        "    ee.ImageCollection(image_collection_name)\n",
        "         .filterBounds(ee.Geometry.Point(lon, lat))\n",
        "         .first()\n",
        ")\n",
        "\n",
        "bands = ['R', 'G', 'B']\n",
        "\n",
        "vizParams = {\n",
        "    'bands': bands, \n",
        "    'min': 0, \n",
        "    'max': 255\n",
        "}\n",
        "\n",
        "# Define a map centered on Blacksburg, VA\n",
        "map = build_map(lat, lon, zoom, vizParams, image, name)\n",
        "\n",
        "# Add the image layer to the map and display it.\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00010-2371f99d-a995-4f33-bfb1-4c9a8d679d2e",
        "deepnote_cell_height": 314,
        "deepnote_cell_type": "markdown",
        "id": "bPVuoWUwHpkw"
      },
      "source": [
        "Expand the image object that is printed to the console by clicking on the dropdown triangles. Expand the property called `bands` and expand one of the bands (0, for example). Note that the CRS transform is stored in the `crs_transform` property underneath the band dropdown and the CRS is stored in the `crs` property, which references an EPSG code. \n",
        "\n",
        "> **EPSG Codes** are 4-5 digit numbers that represent CRS definitions. The acronym EPSG, comes from the (now defunct) European Petroleum Survey Group. The CRS of this image is [EPSG:26917](https://spatialreference.org/ref/epsg/nad83-utm-zone-17n/). You can learn more about these codes from the [EPSG homepage](https://epsg.org/home.html). \n",
        "\n",
        "> The CRS transform is a list `[m00, m01, m02, m10, m11, m12]`in the notation of [this reference](http://docs.oracle.com/javase/7/docs/api/java/awt/geom/AffineTransform.html).  The CRS transform defines how to map pixel coordinates to their associated spherical coordinate through an affine transformation. While affine transformations are beyond the scope of this class, more information can be found at [Rasterio](https://rasterio.readthedocs.io/en/latest/topics/georeferencing.html), which provides detailed documentation for the popular Python library designed for working with geospatial data. \n",
        "\n",
        "In addition to using the dropdowns, you can also access these data programmatically with the `.projection()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00011-16a111b2-2848-483d-ba08-9d10fc0bc06d",
        "deepnote_cell_height": 99,
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1,
        "execution_start": 1654104794334,
        "source_hash": "ce241f95",
        "id": "QPuVBRsCHpkx",
        "outputId": "827d160a-338e-4dbd-f3f3-978802c39761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inspect the projection of band 0: {'type': 'Projection', 'crs': 'EPSG:26917', 'transform': [0.6, 0, 549629.3999999999, 0, -0.6, 4123089]}\n"
          ]
        }
      ],
      "source": [
        "# Display the projection of band 0\n",
        "print('Inspect the projection of band 0:', image.select(0).projection().getInfo())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00012-c7d57bb3-80f0-4d0a-92f0-22512da58714",
        "deepnote_cell_height": 140,
        "deepnote_cell_type": "markdown",
        "id": "W0qGN4VEHpky"
      },
      "source": [
        "Note that the projection can differ by band, which is why it's good practice to inspect the projection of individual image bands. If you call `.projection()` on an image for which the projection differs by band, you'll get an error. Exchange the NAIP imagery with the Planet SkySat MultiSpectral image collection, and note that the error occurs because the 'P' band has a different pixel size than the others. Explore the `ee.Projection` docs to learn about useful methods offered by the `Projection` object. To play with projections offline, try [this tool](http://www.giss.nasa.gov/tools/gprojector/)."
      ]
    }
  ],
  "metadata": {
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "c6e435af-126f-4456-a3d2-3b8f07f2167b",
    "interpreter": {
      "hash": "ec9e24bde1833ef0ef93e4cf9cecf27c680e00c5d9cebee87bbc6a68c9a6acfb"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('geospatial')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}